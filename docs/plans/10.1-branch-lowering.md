# Branch Lowering Implementation Plan

## Overview

Implement branch lowering for RISC-V 32-bit using a two-phase approach inspired by Cranelift's `MachBuffer` system. First pass records relocations (where branches are and what they target), second pass fixes up PC-relative offsets after all block addresses are known.

## How Cranelift Does It

Cranelift uses a sophisticated `MachBuffer` system with lazy fixup resolution:

1. **Labels**: Uses `MachLabel` to represent block labels (essentially block indices)
2. **Binding**: `bind_label()` binds a label to the current code offset when emitting a block
3. **Fixups**: `use_label_at_offset()` records a fixup (relocation) when emitting a branch
4. **Resolution**: `resolve_label_offset()` resolves a label to an offset (or `UNKNOWN_LABEL_OFFSET` if not yet bound)
5. **Lazy Patching**: When a label is bound, pending fixups are checked and patched if the label is now in range
6. **Veneers**: For out-of-range jumps, veneers (trampolines) are inserted

**Key Insight**: Cranelift patches branches incrementally as labels are bound, not in a separate pass.

## Our Approach: Simplified Two-Pass

For our simpler case, we'll use a cleaner two-pass approach:

1. **Pass 1 (Lowering)**: Lower all blocks, record relocations
2. **Pass 2 (Fixup)**: After all blocks are lowered, compute block addresses and fix up all relocations

This is simpler than Cranelift's lazy system but sufficient for our needs.

## Implementation Details

### 1. Relocation Types (`backend/lower/mod.rs`)

```rust
/// Represents a relocation that needs to be fixed up
struct Relocation {
    /// Instruction index in the buffer where the branch is
    inst_idx: usize,
    /// Target block index
    target_block: u32,
    /// Type of branch (determines how to patch)
    branch_type: BranchType,
}

/// Type of branch instruction for patching
enum BranchType {
    /// Unconditional jump (JAL) - uses Jal20 format
    Jump,
    /// Conditional branch true target (BEQ/BNE/etc) - uses B12 format
    BranchTrue,
    /// Conditional branch false target (JAL if needed) - uses Jal20 format
    BranchFalse,
}
```

### 2. Track Block Addresses (`Lowerer` struct)

Add to `Lowerer`:

```rust
/// Map from block index to instruction index where block starts
block_addresses: Vec<usize>,
/// Relocations that need to be fixed up
relocations: Vec<Relocation>,
```

### 3. Update `lower_block()` (`backend/lower/mod.rs`)

```rust
fn lower_block(&mut self, block_idx: usize, block: &Block) {
    // Record where this block starts (instruction index)
    let block_start = self.inst_buffer.instruction_count();

    // Ensure block_addresses is large enough
    if block_idx >= self.block_addresses.len() {
        self.block_addresses.resize(block_idx + 1, 0);
    }
    self.block_addresses[block_idx] = block_start;

    // Lower all instructions (branches will record relocations)
    for inst in &block.insts {
        self.lower_inst(inst);
    }
}
```

### 4. Update Branch Lowering (`backend/lower/branch.rs`)

**`lower_jump()`:**

```rust
pub fn lower_jump(lowerer: &mut Lowerer, target: u32) {
    let inst_idx = lowerer.inst_buffer_mut().instruction_count();

    // Emit JAL with placeholder offset (0)
    lowerer.inst_buffer_mut().emit(crate::Inst::Jal {
        rd: Gpr::Zero,
        imm: 0, // Placeholder, will be fixed up
    });

    // Record relocation
    lowerer.record_relocation(Relocation {
        inst_idx,
        target_block: target,
        branch_type: BranchType::Jump,
    });
}
```

**`lower_br()`:**

```rust
pub fn lower_br(
    lowerer: &mut Lowerer,
    condition: lpc_lpir::Value,
    target_true: u32,
    target_false: u32,
) {
    let condition_reg = lowerer.get_reg_for_value_required(condition);

    // Emit conditional branch for true target
    let branch_inst_idx = lowerer.inst_buffer_mut().instruction_count();
    lowerer.inst_buffer_mut().emit(crate::Inst::Bne {
        rs1: condition_reg,
        rs2: Gpr::Zero,
        imm: 0, // Placeholder
    });

    // Record relocation for true target
    lowerer.record_relocation(Relocation {
        inst_idx: branch_inst_idx,
        target_block: target_true,
        branch_type: BranchType::BranchTrue,
    });

    // Handle false target
    // For now, always emit a jump (we can optimize fall-through later)
    let jump_inst_idx = lowerer.inst_buffer_mut().instruction_count();
    lowerer.inst_buffer_mut().emit(crate::Inst::Jal {
        rd: Gpr::Zero,
        imm: 0, // Placeholder
    });

    // Record relocation for false target
    lowerer.record_relocation(Relocation {
        inst_idx: jump_inst_idx,
        target_block: target_false,
        branch_type: BranchType::BranchFalse,
    });
}
```

### 5. Add Relocation Recording (`Lowerer`)

```rust
fn record_relocation(&mut self, reloc: Relocation) {
    self.relocations.push(reloc);
}
```

### 6. Add Fixup Method (`Lowerer`)

```rust
fn fixup_relocations(&mut self) {
    for reloc in &self.relocations {
        // Get current instruction address (in instructions, not bytes)
        let current_inst_idx = reloc.inst_idx;

        // Get target block start address
        let target_block_start = self.block_addresses
            .get(reloc.target_block as usize)
            .copied()
            .unwrap_or_else(|| {
                panic!("Relocation references invalid block index {}", reloc.target_block)
            });

        // Calculate PC-relative offset (in instructions)
        // RISC-V offsets are relative to the current instruction
        let offset_insts = (target_block_start as i32) - (current_inst_idx as i32);

        // Get current instruction and update offset
        let insts = self.inst_buffer.instructions();
        let current_inst = &insts[reloc.inst_idx];

        let fixed_inst = match (current_inst, &reloc.branch_type) {
            (Inst::Jal { rd, .. }, BranchType::Jump | BranchType::BranchFalse) => {
                // JAL: offset is in instructions, encoded as imm[20:1]
                // Range: ±1MB (±524288 instructions)
                Inst::Jal { rd: *rd, imm: offset_insts }
            }
            (Inst::Bne { rs1, rs2, .. }, BranchType::BranchTrue) => {
                // Branch: offset is in instructions, encoded as imm[12:1]
                // Range: ±4KB (±2048 instructions)
                Inst::Bne { rs1: *rs1, rs2: *rs2, imm: offset_insts }
            }
            // Handle other branch types (Beq, Blt, Bge) similarly
            _ => panic!("Invalid relocation type for instruction: {:?}", current_inst),
        };

        self.inst_buffer.set_instruction(reloc.inst_idx, fixed_inst);
    }
}
```

### 7. Update `lower_function()` (`backend/lower/mod.rs`)

```rust
pub fn lower_function(mut self) -> InstBuffer {
    // Initialize block addresses vector
    self.block_addresses.resize(self.function.blocks.len(), 0);

    // Phase 1: Lower all blocks (records relocations)
    let blocks: Vec<(usize, Block)> =
        self.function.blocks.iter().cloned().enumerate().collect();
    for (block_idx, block) in blocks {
        self.lower_block(block_idx, &block);
    }

    // Phase 2: Fix up relocations
    self.fixup_relocations();

    // Generate epilogue (after all blocks are lowered)
    crate::backend::abi::gen_clobber_restore(&mut self.inst_buffer, &self.frame_layout);
    crate::backend::abi::gen_epilogue_frame_restore(&mut self.inst_buffer, &self.frame_layout);

    self.inst_buffer
}
```

## RISC-V Branch Offset Encoding

### JAL (Unconditional Jump)

- **Format**: `JAL rd, imm`
- **Offset encoding**: `imm[20:1]` (20-bit signed, in instructions)
- **Range**: ±1MB (±524,288 instructions)
- **Calculation**: `offset = target_pc - current_pc` (in instructions)

### Branch Instructions (BEQ, BNE, BLT, BGE)

- **Format**: `BEQ rs1, rs2, imm`
- **Offset encoding**: `imm[12:1]` (12-bit signed, in instructions)
- **Range**: ±4KB (±2,048 instructions)
- **Calculation**: `offset = target_pc - current_pc` (in instructions)

**Important**: RISC-V offsets are in instructions (multiples of 2 bytes for compressed, 4 bytes for standard), not bytes. Since we're RV32 without compressed, offsets are in 4-byte units.

## Special Cases

### Fall-Through Optimization

For `lower_br()`, we can optimize when `target_false` is the next block:

- Check if `target_false == current_block_idx + 1` (if blocks are in order)
- If yes: don't emit jump, just fall through
- If no: emit `JAL` to `target_false`

**Challenge**: Blocks may not be in execution order. For now, we'll always emit the jump and optimize later.

### Block Ordering

Blocks in `function.blocks` may not be in execution order. We handle this by:

- Using `block_idx` parameter (IR block index, not array position)
- Tracking block start addresses by IR block index
- Relocations reference IR block indices

### Out-of-Range Branches

If a branch offset exceeds the range:

- **B12**: ±2KB range - if exceeded, need to use JAL instead
- **Jal20**: ±1MB range - should be sufficient for most functions

For now, we'll assume offsets fit. Can add range checking later.

## Edge Cases

1. **Self-loops**: Block branches to itself (offset = 0) - handled correctly
2. **Empty blocks**: Blocks with no instructions - block address is still recorded
3. **Unreachable blocks**: Blocks that are never targets - still have addresses, just never used
4. **Large functions**: If function > 1MB, some branches may be out of range - need veneers (future work)

## Testing Strategy

1. **Unit Tests** (`backend/tests/branch_tests.rs`):

   - Simple forward branch (block0 → block1)
   - Simple backward branch (block1 → block0)
   - Conditional branch with both targets
   - Multiple branches in one function
   - Self-loop branch

2. **Integration Tests**:
   - Reuse existing `branch_tests.rs` tests
   - Verify correct execution with emulator
   - Test `test_block_address_recording_and_relocation_fixup`

## Files to Modify

1. **`backend/lower/mod.rs`**:

   - Add `Relocation` struct and `BranchType` enum
   - Add `block_addresses` and `relocations` to `Lowerer`
   - Update `lower_block()` to record block starts
   - Add `record_relocation()` method
   - Add `fixup_relocations()` method
   - Update `lower_function()` to call fixup

2. **`backend/lower/branch.rs`**:
   - Update `lower_jump()` to record relocation
   - Update `lower_br()` to record relocations for both targets
   - Handle conditional branch instruction selection (BNE for != 0)

## Future Optimizations

1. **Fall-through detection**: Detect when false target is next block
2. **Block reordering**: Reorder blocks to minimize branch distances
3. **Long-range branches**: Handle branches beyond ±4KB using veneers
4. **Branch optimization**: Remove unnecessary jumps (e.g., jump to next instruction)

## Success Criteria

1. All existing branch tests pass
2. Forward and backward branches work correctly
3. PC-relative offsets are computed correctly
4. Code is clear and maintainable
5. Performance is acceptable (two-pass is fine for now)

## Reference: Cranelift's Approach

Cranelift uses:

- `MachBuffer` with lazy fixup resolution
- `MachLabel` for block labels
- `LabelUse` enum for different branch types (B12, Jal20, etc.)
- Incremental patching as labels are bound
- Veneers for out-of-range jumps

Our approach is simpler but follows the same general pattern:

- Record relocations during lowering
- Fix up after all addresses are known
- Use instruction indices instead of byte offsets for simplicity
